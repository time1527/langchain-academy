{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1012a788",
   "metadata": {},
   "source": [
    "[![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/breakpoints.ipynb) [![åœ¨ LangChain Academy ä¸­æ‰“å¼€](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239469-lesson-2-breakpoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa16f5-abc8-4ed3-8a71-54837fe46917",
   "metadata": {},
   "source": [
    "# æ–­ç‚¹\n",
    "\n",
    "## å›é¡¾\n",
    "\n",
    "åœ¨ `human-in-the-loop`ï¼ˆäººç±»åœ¨ç¯ï¼‰åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¸¸å¸¸å¸Œæœ›åœ¨å›¾è¿è¡Œæ—¶æŸ¥çœ‹å®ƒçš„è¾“å‡ºã€‚\n",
    "\n",
    "æˆ‘ä»¬å·²ç»é€šè¿‡æµå¼ä¼ è¾“ä¸ºæ­¤æ‰“ä¸‹äº†åŸºç¡€ã€‚\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹ `human-in-the-loop` çš„åŠ¨æœºï¼š\n",
    "\n",
    "(1) `Approval`ï¼ˆæ‰¹å‡†ï¼‰- æˆ‘ä»¬å¯ä»¥ä¸­æ–­æ™ºèƒ½ä½“ï¼Œå‘ç”¨æˆ·å±•ç¤ºçŠ¶æ€ï¼Œå¹¶å…è®¸ç”¨æˆ·æ¥å—æŸä¸ªåŠ¨ä½œ\n",
    "\n",
    "(2) `Debugging`ï¼ˆè°ƒè¯•ï¼‰- æˆ‘ä»¬å¯ä»¥å›æ»šå›¾ï¼Œä»¥å¤ç°æˆ–è§„é¿é—®é¢˜\n",
    "\n",
    "(3) `Editing`ï¼ˆç¼–è¾‘ï¼‰- ä½ å¯ä»¥ä¿®æ”¹çŠ¶æ€\n",
    "\n",
    "LangGraph æä¾›äº†å¤šç§æ–¹å¼æ¥è·å–æˆ–æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€ï¼Œä»¥æ”¯æŒä¸åŒçš„ `human-in-the-loop` å·¥ä½œæµã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»‹ç»[æ–­ç‚¹](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage)ï¼Œå®ƒæä¾›äº†ä¸€ç§åœ¨ç‰¹å®šæ­¥éª¤åœæ­¢å›¾çš„ç®€å•æ–¹æ³•ã€‚\n",
    "\n",
    "ä¹‹åæˆ‘ä»¬ä¼šå±•ç¤ºè¿™å¦‚ä½•å®ç°ç”¨æˆ· `approval`ï¼ˆæ‰¹å‡†ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35842345-0694-4f0a-aa62-7d4898abf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d91f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8b4cd-e3ff-48cc-b7b2-f83fadb1c86b",
   "metadata": {},
   "source": [
    "## é¢å‘äººç±»æ‰¹å‡†çš„æ–­ç‚¹\n",
    "\n",
    "é‡æ–°å›é¡¾æˆ‘ä»¬åœ¨æ¨¡å— 1 ä¸­ä½¿ç”¨è¿‡çš„é‚£ä¸ªç®€å•æ™ºèƒ½ä½“ã€‚\n",
    "\n",
    "å‡è®¾æˆ‘ä»¬æ‹…å¿ƒå·¥å…·çš„ä½¿ç”¨ï¼šæˆ‘ä»¬å¸Œæœ›åœ¨æ™ºèƒ½ä½“è°ƒç”¨ä»»ä½•å·¥å…·ä¹‹å‰å…ˆè·å¾—æ‰¹å‡†ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªéœ€è¦åœ¨ç¼–è¯‘å›¾æ—¶ä¼ å…¥ `interrupt_before=[\"tools\"]`ï¼Œå…¶ä¸­ `tools` æ˜¯æˆ‘ä»¬çš„å·¥å…·èŠ‚ç‚¹ã€‚\n",
    "\n",
    "è¿™æ„å‘³ç€æ‰§è¡Œä¼šåœ¨èŠ‚ç‚¹ `tools` ä¹‹å‰è¢«ä¸­æ–­ï¼Œä¹Ÿå°±æ˜¯æ‰§è¡Œå·¥å…·è°ƒç”¨ä¹‹å‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94d1a90-2fe3-4b2a-a901-3bdb89e37edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a by b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm = ChatTongyi(model=\"qwen-plus\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06feae-d12b-490b-95e7-38cf40b74202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# REACTæ¨¡å¼\n",
    "# Define edges: these determine the control flow\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "# display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a783efac-46a9-4fb4-a1c6-a11b02540448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_0ea5704deedb4799b68e63)\n",
      " Call ID: call_0ea5704deedb4799b68e63\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d49669-b1a5-42c2-bdb8-052da89bd7c4",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥è·å–çŠ¶æ€å¹¶æŸ¥çœ‹ä¸‹ä¸€æ­¥å°†è¦è°ƒç”¨çš„èŠ‚ç‚¹ã€‚\n",
    "\n",
    "è¿™æ˜¯éªŒè¯å›¾å·²è¢«ä¸­æ–­çš„å¥½æ–¹æ³•ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61569596-8342-4a37-9c99-e3a9dccb18ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tools',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fea0fb5-3145-4f34-bcc0-9c9e8972d6b4",
   "metadata": {},
   "source": [
    "ç°åœ¨ä»‹ç»ä¸€ä¸ªä¸é”™çš„å°æŠ€å·§ã€‚\n",
    "\n",
    "å½“æˆ‘ä»¬ä»¥ `None` è°ƒç”¨å›¾æ—¶ï¼Œå®ƒä¼šç›´æ¥ä»ä¸Šä¸€æ¬¡çŠ¶æ€æ£€æŸ¥ç‚¹ç»§ç»­æ‰§è¡Œï¼\n",
    "\n",
    "![breakpoints.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png)\n",
    "\n",
    "ä¸ºäº†æ›´æ¸…æ™°ï¼ŒLangGraph ä¼šé‡æ–°å‘å‡ºå½“å‰çŠ¶æ€ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰å·¥å…·è°ƒç”¨çš„ `AIMessage`ã€‚\n",
    "\n",
    "æ¥ç€å®ƒä¼šæ‰§è¡Œå›¾ä¸­çš„åç»­æ­¥éª¤ï¼Œä»å·¥å…·èŠ‚ç‚¹å¼€å§‹ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å·¥å…·èŠ‚ç‚¹ä½¿ç”¨è¿™ä¸ªå·¥å…·è°ƒç”¨è¿è¡Œï¼Œç„¶åå†æŠŠç»“æœä¼ å›èŠå¤©æ¨¡å‹ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896a5f41-7386-4bfa-a78e-3e6ca5e26641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_0ea5704deedb4799b68e63)\n",
      " Call ID: call_0ea5704deedb4799b68e63\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 2 and 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f91a0c-7cc1-4437-adc7-b36abb29beb1",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬æŠŠè¿™äº›å’Œä¸€ä¸ªæ˜ç¡®çš„ç”¨æˆ·æ‰¹å‡†æ­¥éª¤ç»“åˆèµ·æ¥ï¼Œè¯¥æ­¥éª¤ä¼šæ¥æ”¶ç”¨æˆ·è¾“å…¥ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a0eb50-66e3-4538-8103-207aae175154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_9b921ac23e1145908167c6)\n",
      " Call ID: call_9b921ac23e1145908167c6\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_9b921ac23e1145908167c6)\n",
      " Call ID: call_9b921ac23e1145908167c6\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 2 and 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Get user feedback\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "\n",
    "# Check approval\n",
    "if user_approval.lower() == \"yes\":\n",
    "\n",
    "    # If approved, continue the graph execution\n",
    "    for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8ff8762-6fa1-4373-954a-e7f479ee0efb",
   "metadata": {},
   "source": [
    "### åœ¨ LangGraph API ä¸­ä½¿ç”¨æ–­ç‚¹\n",
    "\n",
    "**âš ï¸ å…è´£å£°æ˜**\n",
    "\n",
    "è‡ªä»å½•åˆ¶è¿™äº›è§†é¢‘ä»¥æ¥ï¼Œæˆ‘ä»¬å·²ç»æ›´æ–°äº† Studioï¼Œä½¿å…¶å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå¹¶åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ã€‚ç°åœ¨æ¨èçš„æ–¹å¼æ˜¯ä»¥è¿™ç§å½¢å¼è¿è¡Œ Studioï¼ˆè€Œä¸æ˜¯åƒè§†é¢‘ä¸­å±•ç¤ºçš„æ¡Œé¢åº”ç”¨ï¼‰ã€‚å…³äºæœ¬åœ°å¼€å‘æœåŠ¡å™¨è¯·æŸ¥çœ‹[è¿™é‡Œ](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server)çš„æ–‡æ¡£ï¼Œå…³äºæœ¬åœ° Studio çš„è¿è¡Œæ–¹å¼è¯·æŸ¥çœ‹[è¿™é‡Œ](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server)ã€‚åœ¨æœ¬æ¨¡å—çš„ `/studio` ç›®å½•ä¸­ï¼Œåœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å¯åŠ¨æœ¬åœ°å¼€å‘æœåŠ¡å™¨ï¼š\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "ä½ åº”è¯¥ä¼šçœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼š\n",
    "```\n",
    "- ğŸš€ API: http://127.0.0.1:2024\n",
    "- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ğŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "åœ¨æµè§ˆå™¨ä¸­è®¿é—® Studio UIï¼š`https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`ã€‚\n",
    "\n",
    "LangGraph API [æ”¯æŒæ–­ç‚¹](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_breakpoint/#sdk-initialization)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2eaf1-6b8b-4d80-9902-98ae5587bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    raise Exception(\n",
    "        \"Unfortunately LangGraph Studio is currently not supported on Google Colab\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1dd890-c216-4802-9e33-b637e491e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the URL of the local development server\n",
    "from langgraph_sdk import get_client\n",
    "\n",
    "client = get_client(url=\"http://127.0.0.1:2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80d969-d065-45d7-8bfc-a403a0a1079b",
   "metadata": {},
   "source": [
    "å¦‚ä¸Šæ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç¼–è¯‘è¿è¡Œäº Studio çš„å›¾æ—¶æ·»åŠ  `interrupt_before=[\"node\"]`ã€‚\n",
    "\n",
    "ä¸è¿‡ï¼Œåœ¨ä½¿ç”¨ API æ—¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥æŠŠ `interrupt_before` ä¼ ç»™ `stream` æ–¹æ³•ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9c5017-3a15-46f6-8edf-3997613da323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '1743058b-054d-4c26-8b99-34fd4b6d12aa', 'example': False}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 2, \"b\": 3}'}}]}, 'response_metadata': {'model_name': 'qwen-plus', 'finish_reason': 'tool_calls', 'request_id': '1946d8b0-dc5c-4ac2-a2e5-2c996a889e32', 'token_usage': {'input_tokens': 350, 'output_tokens': 24, 'total_tokens': 374, 'prompt_tokens_details': {'cached_tokens': 0}}}, 'type': 'ai', 'name': None, 'id': 'run--4ca93d06-8d1f-4e4d-b797-0d4004e95e1b-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "thread = await client.threads.create()\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get(\"messages\", [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64272d1-c6ee-435f-9890-9b6c3525ca6c",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰é‚£æ ·ï¼Œé€šè¿‡ä¼ å…¥ `thread_id` å’Œ `None` ä½œä¸ºè¾“å…¥ï¼Œå°±å¯ä»¥ä»æ–­ç‚¹ç»§ç»­æ‰§è¡Œï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76284730-9c90-46c4-8295-400a49760b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 2, \"b\": 3}'}}]}, 'response_metadata': {'model_name': 'qwen-plus', 'finish_reason': 'tool_calls', 'request_id': '1946d8b0-dc5c-4ac2-a2e5-2c996a889e32', 'token_usage': {'input_tokens': 350, 'output_tokens': 24, 'total_tokens': 374, 'prompt_tokens_details': {'cached_tokens': 0}}}, 'type': 'ai', 'name': None, 'id': 'run--4ca93d06-8d1f-4e4d-b797-0d4004e95e1b-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4ecb9a0b-a9f8-45b0-908e-86b38ae95500', 'tool_call_id': 'call_6d092e97a10a47748ce2c6', 'artifact': None, 'status': 'success'}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'model_name': 'qwen-plus', 'finish_reason': 'stop', 'request_id': 'b703a690-2cb2-4b87-81d4-017210886a58', 'token_usage': {'input_tokens': 389, 'output_tokens': 13, 'total_tokens': 402, 'prompt_tokens_details': {'cached_tokens': 0}}}, 'type': 'ai', 'name': None, 'id': 'run--4403dbd3-6465-49cb-9fdc-f7e0417c61ac-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get(\"messages\", [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
