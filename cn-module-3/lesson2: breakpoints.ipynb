{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1012a788",
   "metadata": {},
   "source": [
    "[![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/breakpoints.ipynb) [![在 LangChain Academy 中打开](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239469-lesson-2-breakpoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa16f5-abc8-4ed3-8a71-54837fe46917",
   "metadata": {},
   "source": [
    "# 断点\n",
    "\n",
    "## 回顾\n",
    "\n",
    "在 `human-in-the-loop`（人类在环）场景下，我们常常希望在图运行时查看它的输出。\n",
    "\n",
    "我们已经通过流式传输为此打下了基础。\n",
    "\n",
    "## 目标\n",
    "\n",
    "现在，让我们讨论一下 `human-in-the-loop` 的动机：\n",
    "\n",
    "(1) `Approval`（批准）- 我们可以中断智能体，向用户展示状态，并允许用户接受某个动作\n",
    "\n",
    "(2) `Debugging`（调试）- 我们可以回滚图，以复现或规避问题\n",
    "\n",
    "(3) `Editing`（编辑）- 你可以修改状态\n",
    "\n",
    "LangGraph 提供了多种方式来获取或更新智能体状态，以支持不同的 `human-in-the-loop` 工作流。\n",
    "\n",
    "首先，我们将介绍[断点](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage)，它提供了一种在特定步骤停止图的简单方法。\n",
    "\n",
    "之后我们会展示这如何实现用户 `approval`（批准）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35842345-0694-4f0a-aa62-7d4898abf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d91f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8b4cd-e3ff-48cc-b7b2-f83fadb1c86b",
   "metadata": {},
   "source": [
    "## 面向人类批准的断点\n",
    "\n",
    "重新回顾我们在模块 1 中使用过的那个简单智能体。\n",
    "\n",
    "假设我们担心工具的使用：我们希望在智能体调用任何工具之前先获得批准。\n",
    "\n",
    "我们只需要在编译图时传入 `interrupt_before=[\"tools\"]`，其中 `tools` 是我们的工具节点。\n",
    "\n",
    "这意味着执行会在节点 `tools` 之前被中断，也就是执行工具调用之前。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94d1a90-2fe3-4b2a-a901-3bdb89e37edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a by b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm = ChatTongyi(model=\"qwen-plus\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06feae-d12b-490b-95e7-38cf40b74202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# REACT模式\n",
    "# Define edges: these determine the control flow\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "# display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a783efac-46a9-4fb4-a1c6-a11b02540448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_0ea5704deedb4799b68e63)\n",
      " Call ID: call_0ea5704deedb4799b68e63\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d49669-b1a5-42c2-bdb8-052da89bd7c4",
   "metadata": {},
   "source": [
    "我们可以获取状态并查看下一步将要调用的节点。\n",
    "\n",
    "这是验证图已被中断的好方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61569596-8342-4a37-9c99-e3a9dccb18ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tools',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fea0fb5-3145-4f34-bcc0-9c9e8972d6b4",
   "metadata": {},
   "source": [
    "现在介绍一个不错的小技巧。\n",
    "\n",
    "当我们以 `None` 调用图时，它会直接从上一次状态检查点继续执行！\n",
    "\n",
    "![breakpoints.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png)\n",
    "\n",
    "为了更清晰，LangGraph 会重新发出当前状态，其中包含带有工具调用的 `AIMessage`。\n",
    "\n",
    "接着它会执行图中的后续步骤，从工具节点开始。\n",
    "\n",
    "我们可以看到工具节点使用这个工具调用运行，然后再把结果传回聊天模型，生成最终回答。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896a5f41-7386-4bfa-a78e-3e6ca5e26641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_0ea5704deedb4799b68e63)\n",
      " Call ID: call_0ea5704deedb4799b68e63\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 2 and 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f91a0c-7cc1-4437-adc7-b36abb29beb1",
   "metadata": {},
   "source": [
    "现在，我们把这些和一个明确的用户批准步骤结合起来，该步骤会接收用户输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a0eb50-66e3-4538-8103-207aae175154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_9b921ac23e1145908167c6)\n",
      " Call ID: call_9b921ac23e1145908167c6\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_9b921ac23e1145908167c6)\n",
      " Call ID: call_9b921ac23e1145908167c6\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 2 and 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Get user feedback\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "\n",
    "# Check approval\n",
    "if user_approval.lower() == \"yes\":\n",
    "\n",
    "    # If approved, continue the graph execution\n",
    "    for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8ff8762-6fa1-4373-954a-e7f479ee0efb",
   "metadata": {},
   "source": [
    "### 在 LangGraph API 中使用断点\n",
    "\n",
    "**⚠️ 免责声明**\n",
    "\n",
    "自从录制这些视频以来，我们已经更新了 Studio，使其可以在本地运行并在浏览器中打开。现在推荐的方式是以这种形式运行 Studio（而不是像视频中展示的桌面应用）。关于本地开发服务器请查看[这里](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server)的文档，关于本地 Studio 的运行方式请查看[这里](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server)。在本模块的 `/studio` 目录中，在终端运行以下命令即可启动本地开发服务器：\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "你应该会看到如下输出：\n",
    "```\n",
    "- 🚀 API: http://127.0.0.1:2024\n",
    "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "在浏览器中访问 Studio UI：`https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`。\n",
    "\n",
    "LangGraph API [支持断点](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_breakpoint/#sdk-initialization)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2eaf1-6b8b-4d80-9902-98ae5587bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    raise Exception(\n",
    "        \"Unfortunately LangGraph Studio is currently not supported on Google Colab\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1dd890-c216-4802-9e33-b637e491e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the URL of the local development server\n",
    "from langgraph_sdk import get_client\n",
    "\n",
    "client = get_client(url=\"http://127.0.0.1:2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80d969-d065-45d7-8bfc-a403a0a1079b",
   "metadata": {},
   "source": [
    "如上所示，我们可以在编译运行于 Studio 的图时添加 `interrupt_before=[\"node\"]`。\n",
    "\n",
    "不过，在使用 API 时，你也可以直接把 `interrupt_before` 传给 `stream` 方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9c5017-3a15-46f6-8edf-3997613da323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '1743058b-054d-4c26-8b99-34fd4b6d12aa', 'example': False}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 2, \"b\": 3}'}}]}, 'response_metadata': {'model_name': 'qwen-plus', 'finish_reason': 'tool_calls', 'request_id': '1946d8b0-dc5c-4ac2-a2e5-2c996a889e32', 'token_usage': {'input_tokens': 350, 'output_tokens': 24, 'total_tokens': 374, 'prompt_tokens_details': {'cached_tokens': 0}}}, 'type': 'ai', 'name': None, 'id': 'run--4ca93d06-8d1f-4e4d-b797-0d4004e95e1b-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
    "thread = await client.threads.create()\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get(\"messages\", [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64272d1-c6ee-435f-9890-9b6c3525ca6c",
   "metadata": {},
   "source": [
    "然后，就像我们之前那样，通过传入 `thread_id` 和 `None` 作为输入，就可以从断点继续执行！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76284730-9c90-46c4-8295-400a49760b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 2, \"b\": 3}'}}]}, 'response_metadata': {'model_name': 'qwen-plus', 'finish_reason': 'tool_calls', 'request_id': '1946d8b0-dc5c-4ac2-a2e5-2c996a889e32', 'token_usage': {'input_tokens': 350, 'output_tokens': 24, 'total_tokens': 374, 'prompt_tokens_details': {'cached_tokens': 0}}}, 'type': 'ai', 'name': None, 'id': 'run--4ca93d06-8d1f-4e4d-b797-0d4004e95e1b-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_6d092e97a10a47748ce2c6', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4ecb9a0b-a9f8-45b0-908e-86b38ae95500', 'tool_call_id': 'call_6d092e97a10a47748ce2c6', 'artifact': None, 'status': 'success'}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'model_name': 'qwen-plus', 'finish_reason': 'stop', 'request_id': 'b703a690-2cb2-4b87-81d4-017210886a58', 'token_usage': {'input_tokens': 389, 'output_tokens': 13, 'total_tokens': 402, 'prompt_tokens_details': {'cached_tokens': 0}}}, 'type': 'ai', 'name': None, 'id': 'run--4403dbd3-6465-49cb-9fdc-f7e0417c61ac-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get(\"messages\", [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
