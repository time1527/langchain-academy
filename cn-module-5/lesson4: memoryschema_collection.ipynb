{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‰ΩøÁî®ÈõÜÂêàÊ®°ÂºèÁöÑËÅäÂ§©Êú∫Âô®‰∫∫\n",
    "\n",
    "## ÂõûÈ°æ\n",
    "\n",
    "Êàë‰ª¨ËÆ©ËÅäÂ§©Êú∫Âô®‰∫∫ÊääËØ≠‰πâËÆ∞ÂøÜ‰øùÂ≠òÂà∞Âçï‰∏ÄÁöÑ[Áî®Êà∑Ê°£Ê°à](https://langchain-ai.github.io/langgraph/concepts/memory/#profile)‰∏≠„ÄÇ\n",
    "\n",
    "ÂêåÊó∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂ∫ì [Trustcall](https://github.com/hinthornw/trustcall)ÔºåÁî®‰∫éÂêëËØ•Ê®°ÂºèÂÜôÂÖ•Êñ∞‰ø°ÊÅØ„ÄÇ\n",
    "\n",
    "## ÁõÆÊ†á\n",
    "\n",
    "ÊúâÊó∂ÔºåÊàë‰ª¨Êõ¥Â∏åÊúõÊääËÆ∞ÂøÜ‰øùÂ≠òÂà∞[ÈõÜÂêà](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_200)ËÄåÈùûÂçï‰∏ÄÊ°£Ê°à„ÄÇ\n",
    "\n",
    "Êé•‰∏ãÊù•Êàë‰ª¨‰ºöÊõ¥Êñ∞ËÅäÂ§©Êú∫Âô®‰∫∫ÔºåËÆ©ÂÆÉ[ÊääËÆ∞ÂøÜ‰øùÂ≠òÂà∞ÈõÜÂêà](https://langchain-ai.github.io/langgraph/concepts/memory/#collection)„ÄÇ\n",
    "\n",
    "ÂêåÊó∂Â±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® [Trustcall](https://github.com/hinthornw/trustcall) Êù•Êõ¥Êñ∞ÈõÜÂêà„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Check if the variable is set in the OS environment\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # If not set, prompt the user for input\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "    # Set the environment variable for the current process\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÂÆö‰πâÈõÜÂêàÊ®°Âºè\n",
    "\n",
    "‰∏éÂÖ∂Âú®Âõ∫ÂÆöÊ°£Ê°àÁªìÊûÑ‰∏≠Â≠òÂÇ®Áî®Êà∑‰ø°ÊÅØÔºåÊàë‰ª¨‰ºöÂàõÂª∫‰∏Ä‰∏™Êõ¥ÁÅµÊ¥ªÁöÑÈõÜÂêàÊ®°ÂºèÔºåÁî®Êù•‰øùÂ≠ò‰∏éÁî®Êà∑‰∫íÂä®Áõ∏ÂÖ≥ÁöÑËÆ∞ÂøÜ„ÄÇ\n",
    "\n",
    "ÊØèÊù°ËÆ∞ÂøÜÂ∞Ü‰Ωú‰∏∫Áã¨Á´ãÊù°ÁõÆ‰øùÂ≠òÔºåÊ†∏ÂøÉ‰ø°ÊÅØÂ≠òÊîæÂú® `content` Â≠óÊÆµ„ÄÇ\n",
    "\n",
    "ËøôÁßçÊñπÂºèËÉΩÂ§üÂä®ÊÄÅÊûÑÂª∫‰∏Ä‰∏™ÂºÄÊîæÂºèÈõÜÂêàÔºåÈöèÁùÄÊàë‰ª¨‰∫ÜËß£Êõ¥Â§öÁî®Êà∑‰ø°ÊÅØËÄå‰∏çÊñ≠Êâ©Â±ï„ÄÇ\n",
    "\n",
    "Êàë‰ª¨ÂèØ‰ª•ÊääÈõÜÂêàÊ®°ÂºèÂÆö‰πâ‰∏∫‰∏Ä‰∏™ [Pydantic](https://docs.pydantic.dev/latest/) ÂØπË±°„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Êàë‰ª¨ÂèØ‰ª•Âà©Áî® LangChain [ËÅäÂ§©Ê®°Âûã](https://python.langchain.com/docs/concepts/chat_models/)Êé•Âè£ÁöÑ [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) ÊñπÊ≥ïÊù•‰øùËØÅÁªìÊûÑÂåñËæìÂá∫„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Memory(content=\"User's name is Lance.\"),\n",
       " Memory(content='Lance likes to bike.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "# Initialize the model\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model = ChatTongyi(model=\"qwen3-max\", temperature=0)\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "memory_collection = model_with_structure.invoke(\n",
    "    [HumanMessage(\"My name is Lance. I like to bike.\")]\n",
    ")\n",
    "memory_collection.memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Êàë‰ª¨ÂèØ‰ª•ÈÄöËøá `model_dump()` Â∞Ü Pydantic Ê®°ÂûãÂÆû‰æãÂ∫èÂàóÂåñ‰∏∫ Python Â≠óÂÖ∏„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"User's name is Lance.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_collection.memories[0].model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÊääÊØèÊù°ËÆ∞ÂøÜÁöÑÂ≠óÂÖ∏Ë°®Á§∫ÂÜôÂÖ•Â≠òÂÇ®„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[1].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Âú®Â≠òÂÇ®‰∏≠Ê£ÄÁ¥¢Ëøô‰∫õËÆ∞ÂøÜ„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['1', 'memories'], 'key': '0003d0a6-e6ce-4161-af59-8b506b8a673f', 'value': {'content': \"User's name is Lance.\"}, 'created_at': '2025-09-29T14:01:31.637685+00:00', 'updated_at': '2025-09-29T14:01:31.637689+00:00', 'score': None}\n",
      "{'namespace': ['1', 'memories'], 'key': '972510d1-252b-419f-a32d-3623777eccc2', 'value': {'content': 'Lance likes to bike.'}, 'created_at': '2025-09-29T14:01:31.637752+00:00', 'updated_at': '2025-09-29T14:01:31.637753+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Search\n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Êõ¥Êñ∞ÈõÜÂêàÊ®°Âºè\n",
    "\n",
    "‰∏ä‰∏ÄËØæÊàë‰ª¨ÊèêÂà∞Êõ¥Êñ∞Ê°£Ê°àÊ®°ÂºèÁöÑÊåëÊàò„ÄÇ\n",
    "\n",
    "ÈõÜÂêàÂêåÊ†∑Â¶ÇÊ≠§ÔºÅ\n",
    "\n",
    "Êàë‰ª¨Â∏åÊúõÊó¢ËÉΩÂêëÈõÜÂêà‰∏≠Ê∑ªÂä†Êñ∞ËÆ∞ÂøÜÔºå‰πüËÉΩÊõ¥Êñ∞Â∑≤Â≠òÂú®ÁöÑËÆ∞ÂøÜ„ÄÇ\n",
    "\n",
    "‰∏ãÈù¢Â±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® [Trustcall](https://github.com/hinthornw/trustcall) Êù•Êõ¥Êñ∞ÈõÜÂêà„ÄÇ\n",
    "\n",
    "ËøôÊó¢ÊîØÊåÅÊñ∞Â¢ûÔºå‰πüÊîØÊåÅ[Êõ¥Êñ∞ÈõÜÂêà‰∏≠ÁöÑÂ∑≤ÊúâÊù°ÁõÆ](https://github.com/hinthornw/trustcall?tab=readme-ov-file#simultanous-updates--insertions)„ÄÇ\n",
    "\n",
    "Êàë‰ª¨Â∞ÜÂÆö‰πâ‰∏Ä‰∏™Êñ∞ÁöÑ Trustcall ÊäΩÂèñÂô®„ÄÇ\n",
    "\n",
    "‰æùÊóßÊèê‰æõÂçïÊù°ËÆ∞ÂøÜÁöÑÊ®°Âºè `Memory`„ÄÇ\n",
    "\n",
    "Ê≠§Â§ñÔºåÈÄöËøá `enable_inserts=True` ÂÖÅËÆ∏ÊäΩÂèñÂô®ÂêëÈõÜÂêàÊèíÂÖ•Êñ∞ËÆ∞ÂøÜ„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    # tool_choice=\"Memory\",\n",
    "    tool_choice=\"auto\",\n",
    "    enable_inserts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Lance.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Lance.\"),\n",
    "    HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\"),\n",
    "]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke(\n",
    "    {\"messages\": [SystemMessage(content=instruction)] + conversation}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_75e95ec2a50e42f79269658b)\n",
      " Call ID: call_75e95ec2a50e42f79269658b\n",
      "  Args:\n",
      "    content: Lance had a nice bike ride in San Francisco this morning.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Lance had a nice bike ride in San Francisco this morning.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_75e95ec2a50e42f79269658b'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call\n",
    "for m in result[\"response_metadata\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  {'content': 'Lance had a nice bike ride in San Francisco this morning.'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [\n",
    "    AIMessage(content=\"That's great, did you do after?\"),\n",
    "    HumanMessage(content=\"I went to Tartine and ate a croissant.\"),\n",
    "    AIMessage(content=\"What else is on your mind?\"),\n",
    "    HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\"),\n",
    "]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = (\n",
    "    [\n",
    "        (str(i), tool_name, memory.model_dump())\n",
    "        for i, memory in enumerate(result[\"responses\"])\n",
    "    ]\n",
    "    if result[\"responses\"]\n",
    "    else None\n",
    ")\n",
    "existing_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trustcall_extractor.invoke(\n",
    "    # {\"messages\": updated_conversation, \"existing\": existing_memories}\n",
    "    {\"messages\": updated_conversation},\n",
    "    {\"existing\": existing_memories},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_554d75674ede439aaae82f69)\n",
      " Call ID: call_554d75674ede439aaae82f69\n",
      "  Args:\n",
      "    content: User is planning to go back to Japan this winter.\n"
     ]
    }
   ],
   "source": [
    "# Messages from the model indicate two tool calls were made\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='User is planning to go back to Japan this winter.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËøôËØ¥ÊòéÊàë‰ª¨ÈÄöËøá `json_doc_id` ÊåáÂÆö‰∫ÜÈõÜÂêà‰∏≠Á¨¨‰∏ÄÊù°ËÆ∞ÂøÜÂπ∂ÂØπÂÖ∂ËøõË°å‰∫ÜÊõ¥Êñ∞„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_554d75674ede439aaae82f69'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call\n",
    "for m in result[\"response_metadata\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith ËøΩË∏™Ôºö\n",
    "\n",
    "https://smith.langchain.com/public/ebc1cb01-f021-4794-80c0-c75d6ea90446/r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÂèØ‰ª•Êõ¥Êñ∞ÈõÜÂêàÊ®°ÂºèÁöÑËÅäÂ§©Êú∫Âô®‰∫∫\n",
    "\n",
    "Áé∞Âú®ÔºåÊàë‰ª¨Êää Trustcall ÈõÜÊàêËøõËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂÆûÁé∞ËÆ∞ÂøÜÈõÜÂêàÁöÑÂàõÂª∫‰∏éÊõ¥Êñ∞„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAFNAJIDASIAAhEBAxEB/8QAHAABAQEBAAMBAQAAAAAAAAAAAAUGBAEDBwII/8QAThAAAQQBAQMGCAgMAwcFAAAAAQACAwQFEQYSIRMWMUFV0RQVIlFhk5TSBzI0UnF0gZEjNTZCU1ShsrPBw9M3dZUIFyQzYqS0Y3OCkrH/xAAaAQEBAAMBAQAAAAAAAAAAAAAAAQIDBAUG/8QALxEBAAECAwYEBgIDAAAAAAAAAAECEQMhkRITMUFRUjJxodEEFBUjYfAzgUKxwf/aAAwDAQACEQMRAD8A/qlERARF6btqCjUls25GxQRN3nPPUP5/QrEXyge5cdjK46tKY7F+pFIOlskzWkfYSpvi6fN6zZd00FJ3/LoMeWat88pHEuPzQdB0celdcez+GjaGsxVAADT5OzuWzZopyqnRcn68e4jtSh7QzvTx7iO1aHtDO9PEWI7Koezs7k8RYjsqh7OzuV+1+fQyPHuI7Voe0M708e4jtWh7QzvTxFiOyqHs7O5PEWI7Koezs7k+1+fQyPHuI7Uoe0M708e4jtWh7QzvTxFiOyqHs7O5PEWI7Koezs7k+1+fQyeRnMSToMpRJ9Fhneu9rmvaHMIc0jUEHUEKf4ixHZVD2dncuV+zteCUz4Z7sZYJBPIj8E/Tho6L4pH0aH0qTGHPCZgyW0XBir77XLV7UfI3q+6JmDi06jg5p62nQ6dfAg6ELvWuqmaZtKCIigIiICIiAoeW/wCM2ixVDeHJRB96VnEF24Q2Po/6nb3/AMQrig2w2vtrj53l2lqpLWboOAc1zXjU+kb33Lbg8Z8p/wBLC8iItSCiZrarD4Wya+QtOZM2PlnsjgklMbNSN9+407jeB4u0HAq2vnO2kb4NorVrHs2jx+TfXjDLWPqG1Xt7u9uskZuuaC0uPF27wPSgvN20xztqJsRpNycdFl43BFIYdxwcdd/d3Q3dbrv72hPkjiCF2YbavDZm2KuPtufO6MysbJBJFyjBpq5he0B44ji3UcQsPbO0lXJz3GYyVuZu7MRQRGCEvgjuMMr3MLuLW8XDTeOhOg1K9WLruZtjsnfr1Np5a8bZoLdnItsu3ZZIwGjkncGN1HF7WhnEcTpwDRXfhLwTKVa1jTayEc1mCvvQ1Jy0cq7TUERnVwGp3BxJAHAlbWN4kja9ocA4AjeaWn7QeI+1fKamNvU/go2RDsdc5bH3qtqxWZA4zNY2YuceT03idDrppqvq0MgliZIA5oe0OAe0tI184PQfQg/SIiCDnyKWWw+QjB3nTilLoB5TJOjU+h4aR9J86vKDtSXSS4arEWmWW/E/dJAO5Hq9x+zd/aFeW2vwUrPAREWpBERAREQFw5nHDJUxGJDFPG9s0EoGpjkb8V2nWOojrBIXcitNU0zeBOoZOOeY07OkGRY0F8Dj8YfOYfzm8OkfboeCnc0Mf+uZ3/Wrn91VsnjKWUhEV+uyZo4tJ4OafO1w4g/QVMfs1q9zos1m4mk6hgt7wH0bwJ/atlsOrnZcn4Gx+P0+WZ7/AFq5/dVuhUjo1I60L53xx6gOnmfM88deL3kuPT1lRubT+3857Q33F45tP7fzntDfcV2KO70ktDQos9zaf2/nPaG+4pWzeLs5OnamnzuZaYrtms3cnaAWxzPYCdW9OjeKbFHd6SWhtln5Nk6Ekj3ut5sFxJIbmbbRx8wEmg+heObT+3857Q33F55tP7fzntDfcTYo7vSS0PzzQx/65nf9auf3V3RMobO448ranbX3id+3akneXEfFBeXOPRwaPsC4+bT+vPZwj6w33V2Y/BUaVgWQySe4Bu+E2ZHSyaegu6Onq0U2cOOdzJ68XXmt3zlr0RhcY+TqwOJ3oYzxJcOgPdw1A6A0DXpVhEWFVW1N0ERFiCIiAiIgIiICIiAiIgLO7B8cLad58pkf/MmH8lolndgfyeefnX7zvvtylBokREBERAREQEREBERAREQEREBERAREQFntgdObTdOq3bB+nwmTVaFZ3YL8n5B83IX2/dbmCDRIiICIiAiIgIiICIiAiIgIpmXyhpyRVqsQsX5gSyIu3WtaCNXvOh0aNR1Ek8B16Szb2o6ocL6yXuW2nBqqi/BbNOizHhe1H6HCesl7k8L2o/Q4T1kvcsvl6usalmnRZjwvaj9DhfWS9yeF7UfocJ6yXuT5errGpZR2rzsGzWz1zMW4LNivVaHPjrNDpCC4DUAkDhrqePQCvn/wOfCJjNpJrWHx9HJCZti5edLLGwRsjlsPkaCQ8nXSQDTTp16uK1Vp+0durNXs1sHJBMwxyMdJLo5pGhB4eZZT4Oti8psJWux4tmLmfbkD3yTSyFwaPitGjRwGp+9Pl6usaln1dFmPC9qP0OE9ZL3J4XtR+hwnrJe5Pl6usalmnRZjwvan9DhPWS9yeF7UfocJ6yXuT5errGpZp0WY8L2o/Q4T1kvcuipl71exBDnIK0Ynfycc9aRzm756GEOAI146HiOrgpOBVEZTE/2WX0RFpQREQEREGWt6/wC8EDq8V/1VYUa3/iEP8r/qqyuuvhT5QsiIiwQRcmQyNXHvqNuS8m63OK0I3Sd+QgkN4DhwaeJ4cExWRq5ak23j5eVrue9gfulvlMeWOGhAPBzSPsQdaIiAiIgIuTF5GrlanhVCXlYOUfFvbpb5THFjhoQDwc0j7EyGRq499RtyXk3W5xWhG6TvyEEhvAcODTxPDgg61Ozvyar9dq/x2Kip2d+TVfrtX+OxZ4fihY4tEiIuRBERAREQZW3/AIgj/K/6qsqNb/xCH+V/1VZXXXwp8oWWQ2ofPf2uw2DNu1Uo2K1izI6tK6GSVzDGGtD26EAb5JAPHQdSlZfwjfxeAo5TIZW6XWXkx3hV/BscBpNMwF+rC9o8kak9IWxzmCx2cjhZkoHSGF2/E+OV8UkZ00O69hDhqOnQ8Vxv2QwbqtOu2k6FlPf5B0E8kUjN86v8trg47x4nU8eta7IwdC9cuUMDHkZuWlp7WPqB3LGY7rGS6DlC1pfprpvEAnTitd8Ff5FV/rdz/wAqVd9bZHCVZGvr0uTDbLbjWNmkDGTNYWB7Wb26PJJB0Gh4a66BfgVsjhI2UtnMPQlx7S6QGfISRuD3vc93Dk38NXE9PX0DRSIsOP4QpLI5uQVblmp4TlooJXwPLHOjMcmrdfTp9+h6QFk8nDcqU9vTDmMuG4MCWgDdkcY3GsyU7zidZBqdNHlwA14cVvIKtvLPhdtFi6kBpzss1TXuvm/CAOGp8hmmgd0cQdfQumxgMZYjy7Jq283LDdujlHDlRyYj8/k+SAOGn3pa4ylYT7TZvaBlzJX6UePjrsrtqWXQBhfCJHSu3SN7i7TytRo3o6VP2VyV/ay/hYsnct1mDBRX3MqzOhMsz5HMLzukagBgIaeHlcQtjktksLkpxNaqP5TkhA4x2JIuUjHQx4Y4b44ng7VezJbMYjIurOsVCx1aIwROrzPgLYzpqzWNzSW8B5J4ehWw+VYLI3W43ZzGReNrdSebJ2J/FcjIprBZZIHl77NG+WXHdIPR1K2ZsrJFs/Hl4LkTYtpmtq+GuY6YwchIW75Y5wJBLhrrqQBqtxHsphYsZBj4KQhrV5HywiKR7HwueSXFjwQ5upcegjgdOhfutsxh61apXhphsdWz4ZF+EeXctoRvucTq46OPxif2BSwsqdnfk1X67V/jsVFTs78mq/Xav8di24fihY4tEiIuRBERAREQZW3/AIgj/K/6qsrlzVCwL8WUx0TJ7McRgfA527yrCQRo7oBBHX06no4Lm8MyvYFr2iH312ZV0xMTHDrC8VNFM8MyvYFr2iD308MyvYFr2iD31Niesax7llNFM8MyvYFr2iD308MyvYFr2iD302J6xrHuWU0UzwzK9gWvaIPfXNRzN2/HJJVwlqRkcr4HHloRo9ji1w4u6iCE2J6xrHuWXEUzwzK9gWvaIPfTwzK9gWvaIPfTYnrGse5ZTRTPDMr2Ba9og99PDMr2Ba9og99Niesax7llNTs78mq/Xav8di/PhmV7Ate0Qe+vEdXI5W1ALtR2Pp15WTkOla98zmnVrfJJAaCASSdToAPOLTGzO1Mxl+YIhpURFxoIiICIiAiIgIiICIiAs7sLxxF13nyuQ/ZblH8lolndgvxDMfnZLIO++5MUGiREQEREBERAREQEREBERAREQEREBERAWe2BGmzbSdPKt23dPnsyn+a0KzuwP5OkfNvXW/dalCDRIiICIiAiIgIiICIiAiKdfzuKoTGG5kKsMw6WPkAcPpHUrTTNU2pi4ooovOrA9rU/WBOdWB7Wp+sC2bnE7Z0W0rSKLzqwPa1P1gTnVge1qfrAm5xO2dC0rSKLzqwPa1P1gTnVge1qfrAm5xO2dC0rSzuwfDB2R83J5Ef97MpW3FrZ3anZPJ4WbMU2C1Futdyg8l4Icwn0BwBXyX/Z3wlDAZDI5naGxWrXGa1qrJHjUD8949B4AH6U3OJ2zoWl/SKKLzqwPa1P1gTnVge1qfrAm5xO2dC0rSKLzqwPa1P1gTnVge1qfrAm5xO2dC0rSKLzqwPa1P1gTnVge1qfrAm5xO2dC0rSKMzajBPcGty1LU+eUBWQQ4Aggg8QQsKqKqfFFksIiLEcmYsPqYi9Zi05SGB8jdRrxDSQouzUEcWEpyMb+EniZNK88XSPcAS5x6SSSqm0n5O5X6rL+4VP2f8AxBjfq0X7gXTh/wAU+a8lBERRBERAREQEREBERAREQEREHh7WvY5j2hzXDQgjUELm2X3o25KtvEw17ZjhaehjDGx+6PQC4gDqGgXUubZz5Tmvro/gRKz4Kv3msLSIi5kTtpPydyv1WX9wqfs9+IMb9Wi/cCobSfk7lfqsv7hU/Z/8QY36tF+4F1Yf8X9/8Xk6MjNLWx9qevDy80UTnsi3t3lHAEhuuh01PDXQrMQbbQT5PZurDVLosxWFgzcpwg3mF0bSNOO9uPHVoR6Vr182r7C5WpiMtHWtUxkBaifinku3YoYZXSRsf5PA+XI06A8CsJuijU23uZDJ1aWOwomNoWJIpHW9xvJRTiLfd5B0BHlADXqHXqJw2kygyWPr4iBro7GeuVJvDLrnF4jbId1p5N24zyd4AdG6G9BJF3B7LS4rM4WeKSE1KGHdjnDU77nl8Tt7o00O47U666noU3mllqzq9qnJRfbr5y1kmRyyPax0UwkbulwaSHAP16CNR0qZjrn22kjhyGRbinPwNCw6tNc5cCQ7rt172x6cWNOup3geB0BWzHEajoXzSf4OY47doV8Rs7cjs232DbvxOdNE17t5zNwN0fpqdDvN4aDThqtc7a3ZmFxidtBhmOYd0sN2IFpHDTTeVj8jiy2016vn72KxuIZblq02XXSSWuSaWuLwW/Ecd7yOHUdTqRpx9GH20lvTYOSxi/BcfmY3OqzOsBzwWxmQ77N3RoLQ7Qhx6OIGq6adDw/aDJZyjbp2cfex0VWF8Mu/q5j5STqBpp5YHAniCuKjsnbhx+xNaeSs7xLGWWtHO0frWdF5HDjxcOnTh9yZj947bWWzBjshYxLq+FyU7a9WyZw6TV50jc+PTyWuOmmjieI1AXNa27twVb94YIvxtHIux00jbQ5RzuVEYcxm7o4EubqC5umunHpXBgvg7ZibONhZiNnZYacgccjJE42pGtOrfI3QA/o8vePn06lSsbJX5Nmcvjmy1eXuZjxhG4uduiPwlkuh8nXe3WkdGmvX1qZjom2zfjhl483jvBrNCGGdsVeflhM2VxYxoJa3R2+3dII04jiQv2/aq5Rsz1c1iY6lnwGW9XEVvlWTCMDfYXbjd1w3m9RHHgToufafZCzmsnmZ2WIYWWqNaGBx1LmTQzPlBcNPi6lvQdelLez+azVyS7mTjoJosdYp1Ya0r5G78waHSOc5rSB5LRoAdNTxKZj24fa63bmwZyGHFKnmA41pvCeUIPJiRoc3dGhc3f04/m+c6C3s/mBmRkXxwGOCtckqRyF2vLbmgc8cOA399vX8XXrWU2vY3FbC4jEunj5wQsr+Lo49XOlsw7nxRwJbrwJPQ1x1Wu2axbcLgaOOa7fNeINe/wCe/pc76S4k/arApLm2c+U5r66P4ES6VzbOfKc39dH8CJZT4av3nCwtIiLmRO2k/J3K/VZf3Cp+z34gxv1aL9wKtmK77eJu1otOUmgfG3Xo1LSB/wDqi7NTxy4WpEx34WCJkUsZ4Oje0AFrh0g6hdOH/FPmvJUREUQREQF6zDETxjZ/9QvYiDw1oaNGgAeYLyiICIiAiIgIiIC5tnPlOa+uj+BEvfI9sbHPkc1jGjUucdAAvRsvvPZkrO4Ww2bZkhcT8dgjYwOHoJYSPOND1qz4Kv3msLSIi5kFPu4XF3pjLcx9SeU9L5Imlx+3RUEVpqmmbxIkc2MH2RR9S3uTmxg+yKPqW9yros99id06reUjmxg+yKPqW9y8c2MH2RR9S3uVhE32J3TqXlI5sYPsij6lvcnNjB9kUfUt7lXRN9id06l5SObGD7Io+pb3KFsdgcTax12Sxjakrm5G5G0uiadGssSNaPoAaAtos7sHxwdk/OymRP8A3kyb7E7p1Ly6+bGD7Io+pb3JzYwfZFH1De5V0TfYndOpeUjmxg+yKPqW9y8c2MH2RR9S3uVhE32J3TqXlI5sYPsij6lvcnNjB9kUfUt7lXRN9id06l5SW7NYRrg5uJoaj/0Gn+SrdHQiLGquqrxTdBERYgiIgIiICIiAiIgLO7A/k6T869dd99qUrRLPbAnXZtuunC3bb0eazKEGhREQEREBERAREQEREBERAREQEREBERAWd2C/EEw+bkcg37rkwXt25yOTxOyWTyGCrw2chWi5WOKYEtcAQXcAQfi72nHp0Xyv4Adtto9qcjeqzVcdFh68k1qaVkT98yzyvkDGkv0A3nOPQeDfPxQfcUREBERAREQEREBERAREQERcOdyLMTh7d543uRYS1vzndDR9pIH2q00zVMRHMce0G0EWJ3YY4jZuP0Iha7dDW/Oe7jujp6iT1Dp0yMmbz88m+7Ix1wR/y69dpaPtfvErhiEp3pLMhlsynflkP5zj0/Z1AdQAC9i9zB+Fw8OM4vJd0+Ns32zP6iH3E8bZvtif1EPuLmRbd1h9saQXdByubI0OYmI/9iH3FI2boybM1p6+DtyVIp5TNI1sUbt556Tq5pOnDo6B1LvRN1h9saQXdPjbN9sT+oh9xePG2b7Zn9RD7ilWcjDXydKi9shltiQsIA3RuAE68fSu1Iw8Of8AGNILvectnRoWZmTX/rrxEfsaFbxG10jZmwZuKKNr3BrLUOoj1PAB4J1b9OpHHqWdXhzQ5pa4AtI0IPQQsK/h8KuLTTEeWRd9TRZTYK+99axjJiS6lu8k4ni6J2u6OJ47pBb9AC1a8TFw5wq5onkCIi1giIgIiICzfwgb3NtxBAjE8Jk1+byjf56LSLkzFCPKYuzSm4MnYWa6a7p6j9h0P2LZg1xRiU1TylYfOEXrYJYpJK1sBtuA7krfT1OHoPSFwXIsy6y80rePjr8N1stV73Dhx1IkAPHXqX0MzleM2Kmsp8IJnNfFQxxtlrzXWsnjfMYmSDdcWtc4A6NLgOridB1qjyO0P6/ifYZP7y6qle7JHNHmJKNqJ4ADIq7mDTr3g57terzLCq9cbNrDA5CvPHj8pA9lSpVNyiGVKlsy+DvMrQ7TyW7mo3SAB0gldW0kfiS1no8KwVGnDCYtgG6N4SOBfoPzt0nj08FuIcXj4a3g0NGrHX3xJyTIWhm8CCDpppqCBx9C95rwOnMxhjMxZyZkLRvFuuu7r5tepa9zlx/c/cYxtXF09stm4sRyTGOrWJCyJ2oLS1u68jznjx69PQtwpniirVic7EU8fUtAksk8GGjSdNTo3dPEekL0cjtD+v4n2KT+6s6Ymi+WgtIovI7Q/r+J9hk/vKvLIyGN0krg1jRqSepbImZ5CpsUHc8J3DXdFDQ/Tyg0/mvoKzWw+Lmp1LFy4x8dm44O5N3AxxtB3AR1HiSfp06lpV4nxlcV4szHJlIiIuVBERAREQEREEnPYKtmI2mRz4bUYIjsRnym9ehHQ4cOg/sWUm2Zztcu5PwG4wHRpD3ROI85BBH7V9BRdOF8ViYUWjgt3zfxJtB2ZB7WO5ePEm0HZkHtY7l9JRbvqGJ0j19zLo+b+JNoOzIPax3J4k2g7Mg9rHcvpCJ9QxOkevuZdHzfxJtB2ZB7WO5PEm0HZkHtY7l9IRPqGJ0j19zLo+dRbP5+Ulpp04DpwdJZJH3Nbqr2E2UZWnit5OcW7MZ3mMa3dijdoOIHEuIOuhJ+gArToteJ8bi1xbh5FxERciCIiAiIg//Z",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import merge_message_runs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# Initialize the model\n",
    "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model = ChatTongyi(model=\"qwen3-max\", temperature=0)\n",
    "\n",
    "\n",
    "# Memory schema\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    # tool_choice=\"Memory\",\n",
    "    tool_choice=\"auto\",\n",
    "    # This allows the extractor to insert new memories\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "\n",
    "You have a long term memory which keeps track of information you learn about the user over time.\n",
    "\n",
    "Current Memory (may include updated memories from this conversation): \n",
    "\n",
    "{memory}\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user. \n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = (\n",
    "        [\n",
    "            (existing_item.key, tool_name, existing_item.value)\n",
    "            for existing_item in existing_items\n",
    "        ]\n",
    "        if existing_items\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    updated_messages = list(\n",
    "        merge_message_runs(\n",
    "            messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke(\n",
    "        # {\"messages\": updated_messages, \"existing\": existing_memories}\n",
    "        {\"messages\": updated_messages},\n",
    "        {\"existing\": existing_memories},\n",
    "    )\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    # Â¶ÇÊûúrmeta‰∏≠Êúâjson_doc_idÔºåÂ∞±Áî®ÂÆÉ‰Ωú‰∏∫key„ÄêÊõ¥Êñ∞„ÄëÔºåÂê¶ÂàôÁîüÊàê‰∏Ä‰∏™Êñ∞ÁöÑuuid„ÄêÊèíÂÖ•„Äë\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! Nice to meet you. How's your day going?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That‚Äôs awesome, Lance! San Francisco is such a cool city to bike through‚Äîthough I‚Äôve heard the hills can be quite a workout! Do you have a favorite route or spot you like to ride to?\n"
     ]
    }
   ],
   "source": [
    "# User input\n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['memories', '1'], 'key': '7d3d0c34-3ce0-498e-a264-22392ec09d35', 'value': {'content': 'User introduced themselves as Lance.'}, 'created_at': '2025-09-29T14:01:48.136125+00:00', 'updated_at': '2025-09-29T14:01:48.136126+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': 'a6759364-0310-4185-be9d-cbd17e23b73f', 'value': {'content': 'Lance enjoys biking around San Francisco.'}, 'created_at': '2025-09-29T14:01:54.037339+00:00', 'updated_at': '2025-09-29T14:01:54.037340+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also enjoy going to bakeries\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice! Biking and bakeries‚Äîa perfect combo. San Francisco has some fantastic spots for pastries and fresh bread. Do you have a favorite bakery you like to pedal to? Maybe Tartine, B. Patisserie, or somewhere more hidden? ü•êüö≤\n"
     ]
    }
   ],
   "source": [
    "# User input\n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to bakeries\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Âú®Êñ∞Á∫øÁ®ã‰∏≠ÁªßÁª≠ÂØπËØù„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What bakeries do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you enjoy biking around San Francisco and visiting bakeries, here are a few local favorites that are perfect to explore on two wheels‚Äîeach with great charm and delicious treats:\n",
      "\n",
      "1. **Tartine Bakery (Mission District)**  \n",
      "   A San Francisco institution! Famous for its morning buns, country bread, and croissants. It can get busy, but it‚Äôs worth the stop‚Äîespecially if you time it right after a ride through Dolores Park.\n",
      "\n",
      "2. **Arizmendi Bakery (Inner Sunset)**  \n",
      "   A worker-owned cooperative offering amazing sourdough pizza and pastries. Their seasonal fruit galettes and morning buns are standouts. Plus, it‚Äôs near Golden Gate Park‚Äîgreat for a post-ride snack.\n",
      "\n",
      "3. **B. Patisserie (Pacific Heights)**  \n",
      "   Known for its kouign-amann (a buttery, caramelized pastry), this place is a must for pastry lovers. It‚Äôs a bit more upscale but totally worth it. You could bike there along the Panhandle or from the Presidio.\n",
      "\n",
      "4. **Noe Valley Bakery (Noe Valley)**  \n",
      "   Cozy neighborhood spot with excellent cookies, cupcakes, and scones. Friendly vibe and less touristy‚Äîperfect if you‚Äôre biking through the quieter residential parts of the city.\n",
      "\n",
      "5. **Craftsman and Wolves (Multiple locations, including the Mission)**  \n",
      "   Modern, innovative pastries like the ‚ÄúRebel Within‚Äù (a savory muffin with a soft-boiled egg inside). Great coffee too‚Äîideal for a mid-ride refuel.\n",
      "\n",
      "Want suggestions based on a specific bike route you like, or a favorite type of pastry? üö¥‚Äç‚ôÇÔ∏èü•ê\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input\n",
    "input_messages = [HumanMessage(content=\"What bakeries do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['memories', '1'], 'key': '7d3d0c34-3ce0-498e-a264-22392ec09d35', 'value': {'content': 'User introduced themselves as Lance.'}, 'created_at': '2025-09-29T14:01:48.136125+00:00', 'updated_at': '2025-09-29T14:01:48.136126+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': 'a6759364-0310-4185-be9d-cbd17e23b73f', 'value': {'content': 'Lance enjoys biking around San Francisco.'}, 'created_at': '2025-09-29T14:01:54.037339+00:00', 'updated_at': '2025-09-29T14:01:54.037340+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': '35c82ec0-9b9c-4e3f-954a-407a4a4e3180', 'value': {'content': 'Lance enjoys biking around San Francisco and visiting bakeries.'}, 'created_at': '2025-09-29T14:02:07.080867+00:00', 'updated_at': '2025-09-29T14:02:07.080868+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': '816f1b12-b57a-4f1d-a19b-da8f829bb834', 'value': {'content': 'The user enjoys biking around San Francisco and visiting bakeries.'}, 'created_at': '2025-09-29T14:02:24.253175+00:00', 'updated_at': '2025-09-29T14:02:24.253176+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "\n",
    "https://smith.langchain.com/public/c87543ec-b426-4a82-a3ab-94d01c01d9f4/r\n",
    "\n",
    "## Studio\n",
    "\n",
    "![Screenshot 2024-10-30 at 11.29.25 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6732d0876d3daa19fef993ba_Screenshot%202024-11-11%20at%207.50.21%E2%80%AFPM.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
